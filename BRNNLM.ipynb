{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/dU error norm = 9.6e-10 [ok]\n",
      "    U dims: [10, 100] = 1000 elem\n",
      "grad_check: dJ/dRH error norm = 4.689e-10 [ok]\n",
      "    RH dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dLH error norm = 7.747e-10 [ok]\n",
      "    LH dims: [50, 50] = 2500 elem\n"
     ]
    }
   ],
   "source": [
    "from brnnlm import BRNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = BRNNLM(L0 = wv_dummy, U0 = wv_dummy,\n",
    "              alpha=0.005)\n",
    "model.grad_check(array([1,2,3,4]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data_utils.ner as ner\n",
    "wvT, word_to_numT, num_to_wordT = ner.load_wv('data/vocabTwitter.txt',\n",
    "                                           'data/wordVectorTwitter.txt')\n",
    "wvW, word_to_numW, num_to_wordW = ner.load_wv('data/vocabWiki.txt',\n",
    "                                              'data/wordVectorWiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordvector_neighbors(idxs, wordVecs, num_to_word, n=10):\n",
    "    res_list = []\n",
    "    for idx in idxs:\n",
    "        #print square(wordVecs - wordVecs[idx]).shape\n",
    "        res = argsort(sum(square(wordVecs - wordVecs[idx]), axis=1))[:n+1]\n",
    "        res_list.append([num_to_word[x] for x in res])\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['obama', 'romney', 'barack', 'president', 'clinton', 'hillary', 'potus', 'biden', 'says', 'bill', 'bush']]\n",
      "[['obama', 'barack', 'bush', 'clinton', 'mccain', 'dole', 'gore', 'hillary', 'rodham', 'kerry', 'biden']]\n"
     ]
    }
   ],
   "source": [
    "print wordvector_neighbors([word_to_numT['obama']], wvT, num_to_wordT)\n",
    "print wordvector_neighbors([word_to_numW['obama']], wvW, num_to_wordW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 7861 words from 38444 (91.76% of all tokens)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep\n",
    "vocabsize = 8000\n",
    "        \n",
    "words = list(set(vocab.index[:vocabsize]).intersection(set(word_to_numW.keys())))\n",
    "words.append('UUUNKKK')\n",
    "words.append('<s>')\n",
    "words.append('</s>')\n",
    "vocabsize = len(words)\n",
    "num_to_word = dict(enumerate(words))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.']\n",
      "['we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print [num_to_word[x] for x in X_train[10]]\n",
    "print [num_to_word[y] for y in Y_train[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $W_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "hdim = 50 # dimension of hidden layer = dimension of word vectors\n",
    "L0 = zeros((vocabsize, hdim))\n",
    "for i in xrange(vocabsize-3):\n",
    "    L0[i] = wvW[word_to_numW[num_to_word[i]]]\n",
    "L0[-3] = random.randn(hdim) * sqrt(0.1) # UUUNKKK\n",
    "L0[-2] = random.randn(hdim) * sqrt(0.1) # <s>\n",
    "L0[-1] = random.randn(hdim) * sqrt(0.1) # </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed (optional)\n",
    "ntrain = len(Y_train)\n",
    "X = X_train[:ntrain]\n",
    "Y = Y_train[:ntrain]\n",
    "S = S_train[:ntrain]\n",
    "\n",
    "def randomSchedule(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "        yield random.randint(0, len(Y))\n",
    "        \n",
    "def randomMinibatchSchedule(n, k):\n",
    "    i = 0\n",
    "    while i < n / k:\n",
    "        i += 1\n",
    "        yield [random.randint(0, len(Y)) for _ in xrange(k)]\n",
    "        \n",
    "def annealingAlpha(n, alpha, tau):\n",
    "    for i in xrange(n):\n",
    "        yield alpha * tau / max(i, tau) \n",
    "\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56522\n"
     ]
    }
   ],
   "source": [
    "print ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 9.85084\n",
      "  [2000]: mean loss 5.62363\n",
      "  [4000]: mean loss 5.44422\n",
      "  [6000]: mean loss 5.35979\n",
      "  [8000]: mean loss 5.45166\n",
      "  Seen 10000 in 4901.68 s\n",
      "  [10000]: mean loss 5.26666\n",
      "  [12000]: mean loss 5.21326\n",
      "  [14000]: mean loss 5.23927\n",
      "  [16000]: mean loss 5.24895\n",
      "  [18000]: mean loss 5.12286\n",
      "  Seen 20000 in 9606.30 s\n",
      "  [20000]: mean loss 5.16385\n",
      "  [22000]: mean loss 5.08791\n",
      "  [24000]: mean loss 5.10743\n",
      "  [26000]: mean loss 5.09146\n",
      "  [28000]: mean loss 5.06347\n",
      "  Seen 30000 in 14313.09 s\n",
      "  [30000]: mean loss 5.13498\n",
      "  [32000]: mean loss 5.00622\n",
      "  [34000]: mean loss 5.01644\n",
      "  [36000]: mean loss 4.99323\n",
      "  [38000]: mean loss 5.00177\n",
      "  [40000]: mean loss 5.02402\n",
      "SGD complete: 40000 examples in 19845.38 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 9.8508386909235757),\n",
       " (2000, 5.6236279208727389),\n",
       " (4000, 5.4442216623425219),\n",
       " (6000, 5.3597933901338584),\n",
       " (8000, 5.4516570695163953),\n",
       " (10000, 5.2666633991930523),\n",
       " (12000, 5.2132622515440685),\n",
       " (14000, 5.2392709906749495),\n",
       " (16000, 5.248954177129594),\n",
       " (18000, 5.1228569123401702),\n",
       " (20000, 5.1638487274959477),\n",
       " (22000, 5.0879067970156058),\n",
       " (24000, 5.1074259545429213),\n",
       " (26000, 5.0914607249647537),\n",
       " (28000, 5.0634699288345901),\n",
       " (30000, 5.134978915883905),\n",
       " (32000, 5.0062231888372137),\n",
       " (34000, 5.0164439650648598),\n",
       " (36000, 4.9932292603385955),\n",
       " (38000, 5.0017743556265231),\n",
       " (40000, 5.0240160779608969)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "\n",
    "#n = ntrain\n",
    "n = 40000\n",
    "k = 5\n",
    "model = BRNNLM(L0, U0 = L0, alpha=0.1)\n",
    "# train on S not X, Y is ignored\n",
    "model.train_sgd(X=S, y=Y, idxiter=randomSchedule(n),costevery=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.03987517448\n"
     ]
    }
   ],
   "source": [
    "# compute loss on S not X, Y is ignored\n",
    "dev_loss = model.compute_mean_loss(S_dev, Y_dev)\n",
    "print dev_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_parameters() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-316cdbfbaad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdim_50_vdim_7861_alpha_01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_parameters() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "model.save_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "def adjust_loss(loss, funk):\n",
    "    return (loss + funk * log(funk))/(1 - funk)\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BRNNLM(L0, U0 = L0, alpha=0.1)\n",
    "model.load_parameters('hdim_50_vdim_7861_alpha_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = BRNNLM(zeros((8000,100)))\n",
    "model2.load_parameters('hdim_100_vdim_8000_alpha_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word_all[s] for s in seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wv_all = zeros((wvW.shape[0]+3, wvW.shape[1]))\n",
    "wv_all[range(L0.shape[0])] = L0\n",
    "num_to_word_all = dict()\n",
    "for k, v in num_to_word.items():\n",
    "    num_to_word_all[k] = v\n",
    "i = L0.shape[0]\n",
    "for k, v in word_to_numW.items():\n",
    "    if k not in words:\n",
    "        wv_all[i] = wvW[v]\n",
    "        num_to_word_all[i] = k\n",
    "        i += 1\n",
    "word_to_num_all = du.invert_dict(num_to_word_all)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400003\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400003\n"
     ]
    }
   ],
   "source": [
    "print len(word_to_num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_missing(before, after, n, word_to_num, wv):\n",
    "    before_l = []\n",
    "    for x in before.split():\n",
    "        if x in word_to_num:\n",
    "            before_l.append(word_to_num[x])\n",
    "        else:\n",
    "            before_l.append(word_to_num['UUUNKKK'])\n",
    "    after_l = []\n",
    "    for x in after.split():\n",
    "        if x in word_to_num:\n",
    "            after_l.append(word_to_num[x])\n",
    "        else:\n",
    "            after_l.append(word_to_num['UUUNKKK'])\n",
    "    seqs = model.generate_missing_seqs(before_l, after_l, n, wv, nres=10)\n",
    "    for i in xrange(len(seqs)):\n",
    "        print \" \".join(seq_to_words(seqs[i][0])), seqs[i][1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> barack obama , profession politician . </s> 26.6944394325\n",
      "<s> barack obama that profession politician . </s> 27.128427522\n",
      "<s> barack obama UUUNKKK profession politician . </s> 27.3278067863\n",
      "<s> barack obama is profession politician . </s> 27.3630504739\n",
      "<s> barack obama a profession politician . </s> 27.516876334\n",
      "<s> barack obama the profession politician . </s> 27.5978415799\n",
      "<s> barack obama of profession politician . </s> 27.9931943526\n",
      "<s> barack obama to profession politician . </s> 28.0022687714\n",
      "<s> barack obama an profession politician . </s> 28.1298082475\n",
      "<s> barack obama are profession politician . </s> 28.1502471482\n"
     ]
    }
   ],
   "source": [
    "fill_missing(\"<s> barack obama\", \"profession politician . </s>\", 1, word_to_num_all, wv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DGDG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-48da5789d58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m  \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"birth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DGDG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_rank_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DGDG'"
     ]
    }
   ],
   "source": [
    "before = [word_to_num[x] for x in [\"<s>\", \"he\"]]\n",
    "after = [word_to_num[x] for x in [  \"date\", \"of\", \"birth\", \"DGDG\", \".\", \"</s>\"]]\n",
    "seqs, Js, Ps = model2.generate_rank_missing(before, after, nres=10)\n",
    "for i in xrange(len(seqs)):\n",
    "    print \" \".join(seq_to_words(seqs[i])), Ps[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    print word_to_num['UUUNKKK']\n",
    "    for i in range(len(words)):\n",
    "        if 'UUUNKKK' == words[i]:\n",
    "            idx = multinomial_sample(vocab.freq.values)\n",
    "            words[i] = vocab.index[idx]\n",
    "    #### END YOUR CODE ####\n",
    "    return words\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
